---
id: tf-mnist
title: Handwritten digits recognition with Tensorflow
author: Thomas Barras
tags: [AI]
---

We will build a simple neural network to recognize handwritten digits (MNIST dataset) using **Tensorflow**.

If you have time and motivation I recommend watching this inspirational <a href="https://www.youtube.com/watch?v=vq2nnJ4g6N0">video</a>.

## Problem category

- **Classification**: we learn from data and infer the class of new data
- **Supervised learning**: the dataset is **labeled**

## Data

We'll use the famous MNIST handwritten digit image datasets. There are 60,000 training examples and 10,000 test examples.
You can find the MNIST dataset on <a href="http://yann.lecun.com/exdb/mnist/">Yann Lecun site.</a>
The images are all 28x28 representing numbers from 0 to 9.

Classes are digits from 0 to 9. Given a new image we will try to recognize which digit it represents.

<figure>
  <img src="/img/2016-12-10-tf-mnist/mnist-2.png" height="100" width="100" alt="2 digit image" />
  <figcaption>It's a 2 !</figcaption>
</figure>

## Model

As we want to compute the class of the digit, we need an output layer of 10 coefficients, each of it being the probability of being 0 or 1 or 2...

We will consider **every pixel** of the image : a 28x28 pixel matrix.

The activation function is a logistic regression : y = ax + b.

Now we have a model, that takes an input value, consider a bias, and calculates the probability of having one digit.

Given Y as output vector, X as input vector, W the weight matrix and b a bias vector :

<img src="/img/2016-12-10-tf-mnist/linear.png" height="50" alt="linear output" />

## Activation function: Softmax

Softmax is a simple function that squashes the range of a vector to [0,1]. It's easier to deal with probabilities with it.

In neural network world we call this an **activation function**. All these functions are **non-linear**.

<img  src="/img/2016-12-10-tf-mnist/softmax.png" height="50" alt="softmax" />

## Gradient descent

Gradient descent is an algorithm to minimize a function of a Hilbert space.

Intuitively, if you want to minimize a function, the shortest path is following the opposite of the local gradient.

At each iteration we go to :

<img src="/img/2016-12-10-tf-mnist/gradient_descent.png" height="50" alt="gradient descent" />

## Code

Resources :

- <a href="https://www.tensorflow.org/api_docs/python/">TensorFlow API</a>
- <a href="https://github.com/tensorflow/tensorflow">TensorFlow Github repo</a>: contains a lot of tutorials. Most of my code comes from there.

First, we must define Y and minimize it through some iterations.

TF placeholders represents a variables that will be feed during the execution. Here we use it to define input
and output vectors.

We will treat _100 images_ simultaneously to have more generalization during the learning process.

Here we flatten the input in 784 to multiply with weights.

To build the model we simply use TF matrix multiplication operator and softmax function.

```python
x = tf.placeholder(tf.float32, [None, 784])
W = tf.Variable(tf.zeros([784, 10]))
b = tf.Variable(tf.zeros([10]))
```

**Note**:
Think of dimensions of vectors and matrix. Remember that you must have the same dimensions when adding 2 vectors and
[a,b]x[b,c] when multiplying.

### Model

```python
y = tf.matmul(x, W) + b
y_ = tf.placeholder(tf.float32, [None, 10])
```

(you've probably noticed the '\_' after 'y'. It represents the correct output vector, that will be compared to the
presumed output 'y', since we know the labels of our supervised examples).

Now we need a way to mark the predicted result: we use an <u>entropy computation</u>.

```python
cross*entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y, y*))
train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)
```

In TF we define a program in 2 steps :

1. model and minimization
2. a **session**

**Note**: Usually, compilers run "main" function and all the program, but TF has been thought to work with multiples GPUs. It defines a computation graph and ONLY run when you ask for it.

```python
sess = tf.InteractiveSession()
tf.global_variables_initializer().run()
```

**Train the model**: We put 100 images at each iteration in our learning function.

```python
for _ in range(1000):
batch_xs, batch_ys = mnist.train.next_batch(100)
sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})
```

Finally we test our model using data we did not use during the training process.

```python
correct*prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y*, 1))
accuracy = tf.reduce*mean(tf.cast(correct_prediction, tf.float32))
print(sess.run(accuracy, feed_dict={x: mnist.test.images,
y*: mnist.test.labels}))
```

Results:
<img src="/img/2016-12-10-tf-mnist/tf-exec.png" alt="results" />

## Conclusion

We built a model with 91.9% accuracy, in less than 20 lines of code, using the most basic neural network (only 2 layers !)... not bad.

In a future post we will see how we can reach 99% accuracy using **deep learning**.
