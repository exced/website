---
id: xor-nn
title: XOR Neural Network
author: Thomas Barras
tags: [AI]
---

[Source](https://github.com/exced/xor_neural_net)

We are going to build a simple neural network from scratch to recognize XOR function.

This is the "hello world" of machine learning, and we'll build it using TensorFlow.

<!--truncate-->

Here is what we want, giving 2 vectors A and B:

|  A  |  B  | A XOR B |
| :-: | :-: | :-----: |
|  0  |  0  |    0    |
|  0  |  1  |    1    |
|  1  |  0  |    1    |
|  1  |  1  |    0    |

Since we know the real outputs of the function, we say that we do <b>supervised learning</b>

## Neural Network model

Input vector is all the combinations of 2 bits ([0,0];[0,1];[1,0];[1,1]).

Output vector is a number between 0 and 1 that represents the probability of being a 0 or 1.

Here is the neural net :

![XOR neural net model](/img/2016-12-15-xor-nn/xor_nn_model.png)

## Code

### Activation functions

We do a logistic regression so we'll use a classical linear function: y = ax + b.

Now have a look at 2 functions: **tanh** and **sigmoid** that transform our output values to probabilities.

### tanh

![Hyperbolic Tangent](https://upload.wikimedia.org/wikipedia/commons/thumb/8/87/Hyperbolic_Tangent.svg/1200px-Hyperbolic_Tangent.svg.png)

```python
import numpy as np

def tanh(x):
  return np.tanh(x)

def tanh_p(x):
  return 1.0 - x\*\*2
```

### sigmoid

![Sigmoid](https://upload.wikimedia.org/wikipedia/commons/9/9d/Sigmoide.PNG)

```python
def sigmoid(x):
  return 1.0/(1.0 + np.exp(-x))

def sigmoid_p(x):
  return sigmoid(x)\*(1.0-sigmoid(x))
```

We can see see that these 2 functions are similarly good to discriminate values to -1 and +1.

## TensorFlow

### create the model

Placeholders are memory emplacement which will be feed during the program.

Variables will be define during the TF session.

We also use the built-in sigmoid function of TensorFlow.

```python
import tensorflow as tf

x* = tf.placeholder(tf.float32, [4, 2]) #correct input
y* = tf.placeholder(tf.float32, [4, 1]) #correct output
b1 = tf.Variable(tf.zeros([2])) #bias
b2 = tf.Variable(tf.zeros([1])) #bias
t1 = tf.Variable(tf.random*uniform([2,2], -1, 1)) #theta1
t2 = tf.Variable(tf.random_uniform([2,1], -1, 1)) #theta2
layer2 = tf.sigmoid(tf.matmul(x*, t1) + b1)
y = tf.sigmoid(tf.matmul(layer2, t2) + b2) #training output

x_feed = [[0,0],[0,1],[1,0],[1,1]]
y_feed = [[0],[1],[1],[0]]
```

### gradient descent

Use a loss function: l2 norm works well and minimize it through gradient descent algorithm.

```python
loss = tf.nn.l2*loss(y* - y)
train_step = tf.train.GradientDescentOptimizer(0.5).minimize(loss)
```

### train the model

```python
for i in range(10000):
  sess.run(train*step, feed_dict={x*: x*feed, y*: y_feed})
```

## All the code

```python
from **future** import absolute_import
from **future** import division
from **future** import print_function

import tensorflow as tf

def main(_):

    # Create the model
    x_ = tf.placeholder(tf.float32, [4, 2]) #correct input
    y_ = tf.placeholder(tf.float32, [4, 1]) #correct output
    b1 = tf.Variable(tf.zeros([2])) #bias
    b2 = tf.Variable(tf.zeros([1])) #bias
    t1 = tf.Variable(tf.random_uniform([2,2], -1, 1)) #theta1
    t2 = tf.Variable(tf.random_uniform([2,1], -1, 1)) #theta2
    layer2 = tf.sigmoid(tf.matmul(x_, t1) + b1)
    y = tf.sigmoid(tf.matmul(layer2, t2) + b2) #training output

    x_feed = [[0,0],[0,1],[1,0],[1,1]]
    y_feed = [[0],[1],[1],[0]]

    loss = tf.nn.l2_loss(y_ - y)
    train_step = tf.train.GradientDescentOptimizer(0.5).minimize(loss)

    sess = tf.InteractiveSession()
    tf.global_variables_initializer().run()
    # Train
    for i in range(10000):
      sess.run(train_step, feed_dict={x_: x_feed, y_: y_feed})
      if i % 100 == 0:
        print('y ', sess.run(y, feed_dict={x_: x_feed, y_: y_feed}))
        print('cost ', sess.run(cost, feed_dict={x_: x_feed, y_: y_feed}))

if **name** == '**main**':
tf.app.run(main=main)
```
